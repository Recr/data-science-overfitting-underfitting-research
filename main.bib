% REFERÊNCIAS ALYSON %
@misc{rivery2025,
  author       = {Kevin Bartley},
  title        = {Big data statistics: How much data is there in the world?},
  howpublished = {Blog},
  year         = {2025},
  url          = {https://rivery.io/blog/big-data-statistics-how-much-data-is-there-in-the-world}
}

@misc{google2025,
  author       = {{GOOGLE CLOUD}},
  title        = {O que é machine learning (ML)?},
  howpublished = {Article},
  year         = {s.d.},
  url          = {https://cloud.google.com/learn/what-is-machine-learning?hl=pt-BR}
}

@misc{pedrodomingos2012,
  author       = {Pedro Domingos},
  title        = {A Few Useful Things to Know About Machine Learning},
  howpublished = {Article},
  year         = {2012},
  url          = {https://cacm.acm.org/research/a-few-useful-things-to-know-about-machine-learning}
}

@misc{bashir2020informationtheoreticperspectiveoverfittingunderfitting,
  title         = {An Information-Theoretic Perspective on Overfitting and Underfitting},
  author        = {Daniel Bashir and George D. Montanez and Sonia Sehra and Pedro Sandoval Segura and Julius Lauw},
  year          = {2020},
  eprint        = {2010.06076},
  archiveprefix = {arXiv},
  primaryclass  = {cs.LG},
  url           = {https://arxiv.org/abs/2010.06076}
}

@misc{patel2025,
  author       = {Saumya Patel},
  title        = {Overfitting vs. Underfitting: A Visual Guide for Data Science Beginners},
  howpublished = {Article},
  year         = {2025},
  url          = {https://medium.com/@psaumya567/overfitting-vs-underfitting-a-visual-guide-for-data-science-beginners-4f8b3b961186}
}

@misc{datageeks2025,
  author       = {Pedro César Tebaldi Gomes},
  title        = {Cross Validation: Como Avaliar Modelos em Machine Learning?},
  howpublished = {Article},
  year         = {2025},
  url          = {https://www.datageeks.com.br/cross-validation}
}

@misc{mlmastery2025,
  author       = {Jason Brownlee},
  title        = {How to use Learning Curves to Diagnose Machine Learning Model Performance},
  howpublished = {Article},
  year         = {2019},
  url          = {https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance}
}

@misc{ibm2025_biasvariance,
  author       = {Fanfang Lee},
  title        = {What is bias-variance tradeoff?},
  howpublished = {Article},
  year         = {s.d.},
  url          = {https://www.ibm.com/think/topics/bias-variance-tradeoff}
}

@book{goodfellow-et-al-2016,
  title     = {Deep Learning},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  publisher = {MIT Press},
  note      = {\url{http://www.deeplearningbook.org}},
  year      = {2016}
}

@article{zhalgas2025robustfacerecognition,
  author         = {Zhalgas, Aidana and Amirgaliyev, Beibut and Sovet, Adil},
  title          = {Robust Face Recognition Under Challenging Conditions: A Comprehensive Review of Deep Learning Methods and Challenges},
  journal        = {Applied Sciences},
  volume         = {15},
  year           = {2025},
  number         = {17},
  article-number = {9390},
  url            = {https://www.mdpi.com/2076-3417/15/17/9390},
  issn           = {2076-3417},
  abstract       = {The paper critically reviews face recognition models that are based on deep learning, specifically security and surveillance. Existing systems are susceptible to pose variation, occlusion, low resolution and even aging, even though they perform quite well under controlled conditions. The authors make a systematic review of four state-of-the-art architectures—FaceNet, ArcFace, OpenFace and SFace—through the use of five benchmark datasets, namely LFW, CPLFW, CALFW, AgeDB-30 and QMUL-SurvFace. The measures of performance are evaluated as the area under the receiver operating characteristic (ROC-AUC), accuracy, precision and F1-score. The results reflect that FaceNet and ArcFace achieve the highest accuracy under well-lit and frontal settings; when comparing SFace, this proved to have better robustness to degraded and low-resolution surveillance images. This shows the weaknesses of traditional embedding methods because bigger data sizes reduce the performance of OpenFace with all of the datasets. These results underscore the main point of this study: a comparative study of the models in difficult real life conditions and the observation of the trade-off between generalization and specialization inherent to any models. Specifically, the ArcFace and FaceNet models are optimized to perform well in constrained settings and SFace in the wild ones. This means that the selection of models must be closely monitored with respect to deployment contexts, and future studies should focus on the study of architectures that maintain performance even with fluctuating conditions in the form of the hybrid architectures.},
  doi            = {10.3390/app15179390}
}

@Article{compagnino2025introductionmachinelearning,
AUTHOR = {Compagnino, Antonio Alessio and Maruccia, Ylenia and Cavuoti, Stefano and Riccio, Giuseppe and Tutone, Antonio and Crupi, Riccardo and Pagliaro, Antonio},
TITLE = {An Introduction to Machine Learning Methods for Fraud Detection},
JOURNAL = {Applied Sciences},
VOLUME = {15},
YEAR = {2025},
NUMBER = {21},
ARTICLE-NUMBER = {11787},
URL = {https://www.mdpi.com/2076-3417/15/21/11787},
ISSN = {2076-3417},
ABSTRACT = {Financial fraud represents a critical global challenge with substantial economic and social consequences. This comprehensive review synthesizes the current knowledge on machine learning approaches for financial fraud detection, examining their effectiveness across diverse fraud scenarios. We analyze various fraud types, including credit card fraud, financial statement fraud, insurance fraud, and money laundering, along with their specific detection challenges. The review outlines supervised, unsupervised, and hybrid learning approaches, discussing their applications and performance in different fraud detection contexts. We examine commonly used datasets in fraud detection research and evaluate performance metrics for assessing these systems. The review is further grounded by two case studies applying supervised models to real-world banking data, illustrating the practical challenges of implementing fraud detection systems in operational environments. Through our analysis of the recent literature, we identify persistent challenges, including data imbalance, concept drift, and privacy concerns, while highlighting the emerging trends in deep learning and ensemble methods. This review provides valuable insights for researchers, financial institutions, and practitioners working to develop more effective, adaptive, and interpretable fraud detection systems capable of operating within real-world financial environments.},
DOI = {10.3390/app152111787}
}

@misc{infor2020,
  author       = {{INFOR}},
  title        = {Épocas de crise e supply chain: pandemia exige novos modelos de projeção de demanda},
  howpublished = {Blog},
  year         = {2020},
  url          = {https://www.infor.com/pt-br/blog/seasons-of-crisis-and-supply-chain-pandemic-requires-new-demand-projection-models}
}

% REFERÊNCIAS ELIEL %

@misc{pecanai,
  author       = {PecanAI},
  title        = {Mastering Model Complexity: Avoiding Underfitting and Overfitting Pitfalls},
  howpublished = {Blog},
  year         = {2024},
  month        = {June},
  day          = {13},
  url          = {https://www.pecan.ai/blog/machine-learning-model-underfitting-and-overfitting/},
  note         = {Acesso em: 23 out. 2025}
}

@misc{ultralyticsoverfitting,
  author       = {Ultralytics},
  title        = {Overfitting},
  howpublished = {Blog},
  url          = {https://www.ultralytics.com/glossary/overfitting},
  note         = {Acesso em: 15 out. 2025}
}

@misc{ultralyticsunderfitting,
  author       = {Ultralytics},
  title        = {Underfitting},
  howpublished = {Blog},
  url          = {https://www.ultralytics.com/glossary/underfitting},
  note         = {Acesso em: 15 out. 2025}
}

@inproceedings{nargesian2017learning,
  title={Learning feature engineering for classification.},
  author={Nargesian, Fatemeh and Samulowitz, Horst and Khurana, Udayan and Khalil, Elias B and Turaga, Deepak S},
  booktitle={Ijcai},
  volume={17},
  pages={2529--2535},
  year={2017}
}

@misc{ibm2025_overfittingunderfitting,
  author       = {Tim Mucci},
  title        = {Overfitting vs. underfitting: Finding the balance},
  howpublished = {Blog},
  url          = {https://www.ibm.com/think/topics/overfitting-vs-underfitting},
  note         = {Acesso em: 20 out. 2025}
}

@article{sculley2015hidden,
  title={Hidden technical debt in machine learning systems},
  author={Sculley, David and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Francois and Dennison, Dan},
  journal={Advances in neural information processing systems},
  volume={28},
  year={2015}
}

@misc{cmublog2020overfitting,
  author = {Chenlei Fang and Jerry Ding and Qicheng Huang and Tian Tong and Yijie Sun},
  howpublished = {Blog},
  url = {https://blog.ml.cmu.edu/2020/08/31/4-overfitting/},
  year = {2020},
  title = {4 – The Overfitting Iceberg}
}

@misc{lafon2024understandingdoubledescentphenomenon,
  title={Understanding the Double Descent Phenomenon in Deep Learning}, 
  author={Marc Lafon and Alexandre Thomas},
  year={2024},
  eprint={2403.10459},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  url={https://arxiv.org/abs/2403.10459}
}

@article{belkin2019reconciling,
  doi = {10.1073/pnas.1903070116},
  author = {Mikhail Belkin  and Daniel Hsu  and Siyuan Ma  and Soumik Mandal },
  title = {Reconciling modern machine-learning practice and the classical bias–variance trade-off},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849-15854},
  year = {2019},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.1903070116}
}

@misc{datilio2021regularizacao,
  author = {Lucas Datilio Carderelli},
  howpublished = {Blog},
  title = {O que é Regularização L1 e L2},
  year = {2021},
  url = {https://medium.com/data-hackers/o-que-%C3%A9-regulariza%C3%A7%C3%A3o-l1-l2-6697ada36a51}
}

@Article{electronics11040521,
  author = {Kotsilieris, Theodore and Anagnostopoulos, Ioannis and Livieris, Ioannis E.},
  title = {Special Issue: Regularization Techniques for Machine Learning and Their Applications},
  journal = {Electronics},
  volume = {11},
  year = {2022},
  number = {4},
  article-number = {521},
  url = {https://www.mdpi.com/2079-9292/11/4/521},
  issn = {2079-9292},
  doi = {10.3390/electronics11040521}
}

@misc{srivastava2022regularization,
  author = {Niharika Srivastava},
  howpublished = {Blog},
  title = {Regularization in Deep Learning: L1, L2 e Dropout},
  year = {2022},
  url = {https://www.e2enetworks.com/blog/regularization-in-deep-learning-l1-l2-dropout},
}

@article{srivastava2014dropout,
  doi = {10.5555/2627435.2670313},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  title = {Dropout: a simple way to prevent neural networks from overfitting},
  year = {2014},
  issue_date = {January 2014},
  publisher = {JMLR.org},
  volume = {15},
  number = {1},
  issn = {1532-4435},
  journal = {J. Mach. Learn. Res.},
  pages = {1929–1958},
  numpages = {30},
  keywords = {deep learning, model combination, neural networks, regularization}
}

@article{li2024keeping,
  title={Keeping deep learning models in check: A history-based approach to mitigate overfitting},
  author={Li, Hao and Rajbahadur, Gopi Krishnan and Lin, Dayi and Bezemer, Cor-Paul and Jiang, Zhen Ming},
  journal={IEEE Access},
  volume={12},
  pages={70676--70689},
  year={2024},
  publisher={IEEE}
}

@misc{aws2025overfitting,
  author = {Amazon},
  howpublished = {Blog},
  title = {What is Overfitting?},
  url = {https://aws.amazon.com/what-is/overfitting/},
}
@misc{google2025overfitting,
  author = {Google},
  howpublished = {Blog},
  title = {Overfitting},
  url = {https://developers.google.com/machine-learning/crash-course/overfitting/overfitting?hl=pt-br},
}
@misc{microsoft2025overfitting,
  author = {Microsoft},
  howpublished = {Blog},
  title = {Prevent overfitting and imbalanced data with Automated ML},
  url = {https://learn.microsoft.com/en-us/azure/machine-learning/concept-manage-ml-pitfalls?view=azureml-api-2},
}

@misc{example,
  author       = {},
  title        = {},
  howpublished = {},
  year         = {},
  month        = {},
  day          = {},
  url          = {},
  note         = {}
}